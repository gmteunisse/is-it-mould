{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This is a Jupyter notebook, designed to test out the PRAW library and get an idea of how to use it for reddit scraping. It requires the `praw` library, which can be installed directly (e.g. `pip install praw`) or in a conda environment.\n",
    "\n",
    "# Set up a reddit instance\n",
    "First, we need to set up a reddit instance. This requires setting up a Reddit application. Head over to [the authorized applications page](https://www.reddit.com/prefs/apps) and create a new script at the bottom. It is required to add a title, description and redirect uri (use `http://localhost:8080` here). Once your apps has been created, you will have the required credentials to start using the Reddit API. The `user_agent` is the name of the application; the `client_id` is the line of gibberish next to the icon of the application; and the `client_secret` is the `secret` gibberish. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "reddit = praw.Reddit(\n",
    "    user_agent=\"IsItMould?\",\n",
    "    client_id=\"EXRL43UtmsdPymRwqZErkg\",\n",
    "    client_secret=\"JrkEkIWSzI_srcUvJeh24Q__qvS3VQ\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get and filter data from subreddit\n",
    "Next, let's try to obtain some data from our subreddit of interest: `r/kombucha`. First, lets print the attributes and methods for the latest post. Then we will get the last 5 posts by date and output some basic information like the post id, timestamp and title.\n",
    "\n",
    "## Available attributes\n",
    "Let's have a look at the available attributes and methods for the objects that are returned by `praw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MESSAGE_PREFIX', 'STR_FIELD', 'VALID_TIME_FILTERS', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_convert_to_fancypants', '_create_or_update', '_fetch', '_fetch_data', '_fetch_info', '_fetched', '_kind', '_parse_xml_response', '_path', '_reddit', '_reset_attributes', '_safely_add_arguments', '_submission_class', '_submit_media', '_subreddit_collections_class', '_subreddit_list', '_upload_inline_media', '_upload_media', '_url_parts', '_validate_gallery', '_validate_inline_media', '_validate_time_filter', 'banned', 'collections', 'comments', 'contributor', 'controversial', 'display_name', 'emoji', 'filters', 'flair', 'fullname', 'gilded', 'hot', 'message', 'mod', 'moderator', 'modmail', 'muted', 'new', 'parse', 'post_requirements', 'quaran', 'random', 'random_rising', 'rising', 'rules', 'search', 'sticky', 'stream', 'stylesheet', 'submit', 'submit_gallery', 'submit_image', 'submit_poll', 'submit_video', 'subscribe', 'top', 'traffic', 'unsubscribe', 'widgets', 'wiki']\n",
      "{'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x1124e1720>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='Kombucha'), 'selftext': \"Hey everyone\\nSo, after 3 failed attempts of Kombucha making, I bought a heating mat and a thermostat controller (will do a seperate post on that one). Made my first batch. It was too vinegary but no mold or kahm (its ph was 3.5 and lowered to 2.5), so fixed it with some sweet tea and bottled it. That's going good as I tried some from a bottle. \\n\\nNow on my second batch, which I used the scoby from the first, its day 5 and it had this disgusting rotten smell and taste. I just read that it may happen due to unfiltered watered (which I have used) and I also should look for a better tea brand. Other than that I am sure I clean my utensils very well. But now that rotten taste in it does not allow me to know by when kombucha would be done, as it throws me away immediately, so I am thinking of making a new batch. And the changes on the water quality and tea brand are noted. \\n\\nQuestions time.\\n1. Any other factor you might think of in your experience?\\n2. Can I use the scoby from the last one, or is it better to purchase a new one and start fresh?\", 'author_fullname': 't2_5akll86r', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'My kombucha smells rotten ðŸ˜”', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/Kombucha', 'hidden': False, 'pwls': 6, 'link_flair_css_class': 'one', 'downs': 0, 'thumbnail_height': 0, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_ta7nxf', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 0, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': '', 'secure_media_embed': {}, 'link_flair_text': \"what's wrong!?\", 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1646832123.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.Kombucha', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class=\"md\"><p>Hey everyone\\nSo, after 3 failed attempts of Kombucha making, I bought a heating mat and a thermostat controller (will do a seperate post on that one). Made my first batch. It was too vinegary but no mold or kahm (its ph was 3.5 and lowered to 2.5), so fixed it with some sweet tea and bottled it. That&#39;s going good as I tried some from a bottle. </p>\\n\\n<p>Now on my second batch, which I used the scoby from the first, its day 5 and it had this disgusting rotten smell and taste. I just read that it may happen due to unfiltered watered (which I have used) and I also should look for a better tea brand. Other than that I am sure I clean my utensils very well. But now that rotten taste in it does not allow me to know by when kombucha would be done, as it throws me away immediately, so I am thinking of making a new batch. And the changes on the water quality and tea brand are noted. </p>\\n\\n<p>Questions time.\\n1. Any other factor you might think of in your experience?\\n2. Can I use the scoby from the last one, or is it better to purchase a new one and start fresh?</p>\\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '9b142210-e9d1-11e8-8b0c-0e6cbe9c92dc', 'can_gild': False, 'spoiler': False, 'locked': False, 'call_to_action': '', 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2qx36', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ffb000', 'id': 'ta7nxf', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='cucvogli'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/Kombucha/comments/ta7nxf/my_kombucha_smells_rotten/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/Kombucha/comments/ta7nxf/my_kombucha_smells_rotten/', 'subreddit_subscribers': 324722, 'created_utc': 1646832123.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_comments_by_id': {}}\n"
     ]
    }
   ],
   "source": [
    "# Get the attributes for a subreddit\n",
    "print(dir(reddit.subreddit('kombucha')))\n",
    "\n",
    "# Get the attributes for a post\n",
    "last_posts = reddit.subreddit('kombucha').new(limit=1)\n",
    "for post in last_posts:\n",
    "    print(vars(post))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return the last posts\n",
    "Seems that the last posts can be returned using `.new()`. We can then extract useful information like the post id, the timestamp and the post title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 5 posts from the kombucha reddit:\n",
      "\n",
      "id\ttimestamp\ttitle\n",
      "ta7nxf\t2022-03-09 14:22:03\tMy kombucha smells rotten ðŸ˜”\n",
      "ta4k3h\t2022-03-09 11:01:26\tFirst brew PH stuck at 3.5\n",
      "ta49vo\t2022-03-09 10:39:35\tTips for making Jackfruit Kombucha?\n",
      "ta0dw0\t2022-03-09 06:15:44\tMoving Kombucha\n",
      "ta047p\t2022-03-09 06:00:10\tr/Kombucha Weekly Weird Brews and Experiments (March 09, 2022)\n"
     ]
    }
   ],
   "source": [
    "# Import datetime\n",
    "import datetime\n",
    "\n",
    "# Grab the last 5 posts\n",
    "last_posts = reddit.subreddit('kombucha').new(limit=5)\n",
    "\n",
    "# Function to determine date and time posted\n",
    "# courtesty of https://www.reddit.com/r/learnprogramming/comments/37kr5n/praw_is_it_possible_to_get_post_time_and_date/\n",
    "def get_date(submission):\n",
    "\ttime = submission.created\n",
    "\treturn datetime.datetime.fromtimestamp(time)\n",
    "\n",
    "# Print post id, date and title\n",
    "print(\"Last 5 posts from the kombucha reddit:\\n\")\n",
    "print(\"id\\ttimestamp\\ttitle\")\n",
    "for post in last_posts:\n",
    "    attrs = (post.id, get_date(post), post.title)\n",
    "    print(\"%s\\t%s\\t%s\" % attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter by flair\n",
    "To create our scraper, we are looking for posts that contain images of pellicles. Specifically, we want images of pellicles that have been classified as either \"mold!\" or not \"not mold\" by their flair. It seems to be possible to filter posts by flair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the last 5 mold posts\n",
    "mold_posts = reddit.subreddit('kombucha').search(query='flair:\"mold!\"', sort='new', limit = 5, syntax='lucene')\n",
    "print(\"Last 5 mold posts from the kombucha reddit:\\n\")\n",
    "print(\"id\\ttimestamp\\ttitle\")\n",
    "for post in mold_posts:\n",
    "    attrs = (post.id, get_date(post), post.title)\n",
    "    print(\"%s\\t%s\\t%s\" % attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the last 5 not mold posts\n",
    "not_mold_posts = reddit.subreddit('kombucha').search(query='flair:\"not mold\"', sort='new', limit = 5, syntax='lucene')\n",
    "print(\"\\nLast 5 not mold posts from the kombucha reddit:\\n\")\n",
    "print(\"id\\ttimestamp\\ttitle\")\n",
    "for post in not_mold_posts:\n",
    "    attrs = (post.id, get_date(post), post.title)\n",
    "    print(\"%s\\t%s\\t%s\" % attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That didn't seem to work - the flairs are different, yet the posts overlap. Altering the syntax between `lucene`, `cloudsearch` and `plain` syntaxes also didn't do the trick. On the reddit website - `flair:\"not mold\"` returns just the `not mold` posts. As an alternative, can we get all `mold` posts and then obtain by their flair?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mold_posts = reddit.subreddit('kombucha').search(\n",
    "    query='flair:\"*mold*\"', \n",
    "    sort='new', \n",
    "    limit = 5, \n",
    "    syntax='lucene')\n",
    "print(\"\\nLast 5 mold or not mold posts from the kombucha reddit:\\n\")\n",
    "print(\"id\\ttimestamp\\tflair\\ttitle\")\n",
    "for post in mold_posts:\n",
    "    attrs = (post.id, get_date(post), post.link_flair_text, post.title)\n",
    "    print(\"%s\\t%s\\t%s\\t%s\" % attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That worked! These posts match the results on the reddit website, and this seems sufficient to scrape the posts (and hopefully images) we need for our classifier.\n",
    "\n",
    "## Image from post\n",
    "Next, we need to obtain the URLs to images that have been attached to these posts. First, let's try to get these from the post URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mold_posts = reddit.subreddit('kombucha').search(\n",
    "    query='flair:\"*mold*\"', \n",
    "    sort='new', \n",
    "    limit = 5, \n",
    "    syntax='lucene')\n",
    "print(\"\\nLast 5 mold or not mold posts from the kombucha reddit:\\n\")\n",
    "print(\"id\\ttimestamp\\tflair\\turl\")\n",
    "for post in mold_posts:\n",
    "    attrs = (post.id, get_date(post), post.link_flair_text, post.url)\n",
    "    print(\"%s\\t%s\\t%s\\t%s\" % attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that in some posts, the URL points directly to an image, whereas in others, it points to a gallery. Perhaps the `media_metadata` allows us to access all images within the gallery?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mold_posts = reddit.subreddit('kombucha').search(\n",
    "    query='flair:\"*mold*\"', \n",
    "    sort='new', \n",
    "    limit = 5, \n",
    "    syntax='lucene')\n",
    "print(\"\\nLast 5 mold or not mold posts from the kombucha reddit:\\n\")\n",
    "print(\"id\\ttimestamp\\tflair\\turl\")\n",
    "for post in mold_posts:\n",
    "    #print(dir(post))\n",
    "    #break\n",
    "    attrs = (post.id, get_date(post), post.link_flair_text, post.media_metadata)\n",
    "    print(\"%s\\t%s\\t%s\\t%s\\n\\n\" % attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns an error, because one of the submissions doesn't have `media_metadata`. It seems that this is the post that pointed directly to an URL. Can we avoid this problem by checking if the URL point to an image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mold_posts = reddit.subreddit('kombucha').search(\n",
    "    query='flair:\"*mold*\"', \n",
    "    sort='new', \n",
    "    limit = 5, \n",
    "    syntax='lucene')\n",
    "print(\"\\nLast 5 mold or not mold posts from the kombucha reddit:\\n\")\n",
    "print(\"id\\ttimestamp\\tflair\\turl\")\n",
    "for post in mold_posts:\n",
    "    if post.url.endswith(('.jpg', '.png', '.gif', '.jpeg')):\n",
    "        attrs = (post.id, get_date(post), post.link_flair_text, post.url)\n",
    "    else:\n",
    "        attrs = (post.id, get_date(post), post.link_flair_text, post.media_metadata)\n",
    "    print(\"%s\\t%s\\t%s\\t%s\\n\\n\" % attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! Although it's probably safest to still check whether the `media_metadata` attribute exists once we develop our true scraper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the `media_metadata`\n",
    "Now that we know that the image URLs can be stored in the `media_metadata` attribute, it's time to get the image URLs out. It seems that the the `media_metadata` attribute is a dict, which contains one name:value pair per image, and that for each image, many links to different images sizes are created in the `p` name:value pair. Another link is stored in the `s` key:value pair, which appears not to be scaled, so I assume that this is the original submission. For now, let's select these URLs, although for our classifier purposes, they are much too large, and we may choose to select a resized image (e.g. `x = 1080`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mold_posts = reddit.subreddit('kombucha').search(\n",
    "    query='flair:\"*mold*\"', \n",
    "    sort='new', \n",
    "    limit = 10, \n",
    "    syntax='lucene')\n",
    "print(\"\\nLast 5 mold or not mold posts from the kombucha reddit:\\n\")\n",
    "print(\"id\\ttimestamp\\tflair\\turl\")\n",
    "for post in mold_posts:\n",
    "    if post.url.endswith(('.jpg', '.png', '.gif', '.jpeg')):\n",
    "        attrs = (post.id, get_date(post), post.link_flair_text, post.url)\n",
    "        print(\"%s\\t%s\\t%s\\t%s\" % attrs)\n",
    "    else:\n",
    "        attrs = (post.id, get_date(post), post.link_flair_text)\n",
    "        try:\n",
    "            media_metadata = post.media_metadata\n",
    "            for id in media_metadata.keys():\n",
    "                url = media_metadata[id][\"s\"][\"u\"]\n",
    "                sub_attrs = attrs + (url,)\n",
    "                print(\"%s\\t%s\\t%s\\t%s\" % sub_attrs)\n",
    "        except AttributeError:\n",
    "            # Most likely a video - skip\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and store images\n",
    "Now we have URLs to a bunch of images (sometimes multiple from the same post) and their classification. Let's see if we can download these images using `urllib` requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os.path\n",
    "import time\n",
    "\n",
    "# Downloads an image to path\n",
    "def download_img(url, path, name):\n",
    "    img_path = os.path.join(path, name)\n",
    "    with open(img_path, \"wb\") as f:\n",
    "        f.write(urllib.request.urlopen(url).read())\n",
    "\n",
    "# Simplifies the classification\n",
    "def get_class(flair):\n",
    "    cl = \"mold_1\" if flair == \"mold!\" else \"not_mold_0\"\n",
    "    return(cl)\n",
    "\n",
    "# Define path\n",
    "path = \"/Users/guus/Downloads\"\n",
    "\n",
    "# Get the last 5 mold/not mold posts\n",
    "mold_posts = reddit.subreddit('kombucha').search(\n",
    "    query='flair:\"*mold*\"', \n",
    "    sort='new', \n",
    "    limit = 5, \n",
    "    syntax='lucene')\n",
    "\n",
    "# Loop through posts and save the image\n",
    "for post in mold_posts:\n",
    "    if post.url.endswith(('.jpg', '.png', '.gif', '.jpeg')):\n",
    "        url = post.url\n",
    "        id = post.id\n",
    "        cl = get_class(post.link_flair_text)\n",
    "        img_name, img_type = os.path.splitext(url)\n",
    "        img_name = os.path.basename(img_name)\n",
    "        name = id + \"_\" + img_name + \"_\" + cl + img_type\n",
    "        download_img(url, path, name)\n",
    "        \n",
    "        # 1 request per 2 second allowed (apparently)\n",
    "        time.sleep(2)\n",
    "\n",
    "    else:\n",
    "        attrs = (post.id, get_date(post), post.link_flair_text)\n",
    "        try:\n",
    "            media_metadata = post.media_metadata\n",
    "            for id in media_metadata.keys():\n",
    "                url = media_metadata[id][\"s\"][\"u\"]\n",
    "                post_id = post.id\n",
    "                cl = get_class(post.link_flair_text)\n",
    "                img_type = media_metadata[id][\"m\"].replace(\"image/\", \"\")\n",
    "                img_name = id\n",
    "                name = post_id + \"_\" + img_name + \"_\" + cl + img_type\n",
    "                download_img(url, path, name)\n",
    "                \n",
    "                # 1 request per 2second allowed (apparently)\n",
    "                time.sleep(2)\n",
    "        except AttributeError:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of available datapoints\n",
    "Now that we know that we can download images with a classification, let's inspect how large out dataset will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the last 5 mold/not mold posts\n",
    "mold_posts = reddit.subreddit('kombucha').search(\n",
    "    query='flair:\"*mold*\"', \n",
    "    sort='new', \n",
    "    limit = None, \n",
    "    time_filter = \"all\",\n",
    "    syntax='lucene')\n",
    "\n",
    "n = 0\n",
    "print(\"id\\ttimestamp\\tflair\\ttitle\")\n",
    "for post in mold_posts:\n",
    "    attrs = (post.id, get_date(post), post.link_flair_text, post.title)\n",
    "    print(\"%s\\t%s\\t%s\\t%s\" % attrs)\n",
    "    n += 1\n",
    "print(\"Number of posts:\\t\" + str(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be a limit on the number of posts returned: only 248 posts were returned, but the oldest date is not very far in the past."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving from PRAW to Pushshift\n",
    "Pushshift is a copy of all reddit posts and comments + an API intended for big queries. This means that it can be used to query further back than PRAW allows. Below is an API call that queries the kombucha subreddit for the 5 latest posts. The output is a JSON object with all the familiar attributes.\n",
    "\n",
    "```\n",
    "https://api.pushshift.io/reddit/search/submission/?subreddit=kombucha&sort=desc&sort_type=created_utc&size=5\n",
    "```\n",
    "\n",
    "However, there are a few issues that need to be overcome when using Pushshift:\n",
    "\n",
    "* Pushshift doesn't have any method for filtering by flair, though flairs are available in the output. This means we'll need to device a different way of findings mould related posts;\n",
    "* Pushshift stores posts as they were submitted. This means that any later updated to the post might not be included in the database. Thus, any flair update to `mold!` or `not mold` will not have been included, and they are likely to still be labeled as `what's wrong!?` or `question`.\n",
    "\n",
    "Since we're looking for mold, we can use the query `mold` to find any posts that mention mould. However, these is change of missing many posts. Alternatively, we could scrape all posts, and then look for `.jpg` in the post link or metadata like we did above. We then use PRAW to obtain the flair for those submission IDs.\n",
    "\n",
    "In a way, only having the original flair from Pushshift is an advantage, because some users initially label their question about mould as `mold!`, despite not knowing whether it is mould. Those labels could be wrong if not updated by the user. By filtering those submissions out, we improve our final data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/guus/Documents/Code/is-it-mould/test_scraping.ipynb Cell 27'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/guus/Documents/Code/is-it-mould/test_scraping.ipynb#ch0000040?line=3'>4</a>\u001b[0m q \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhttps://api.pushshift.io/reddit/search/submission/?subreddit=kombucha&sort=desc&sort_type=created_utc&size=500&q=mold\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/guus/Documents/Code/is-it-mould/test_scraping.ipynb#ch0000040?line=4'>5</a>\u001b[0m response \u001b[39m=\u001b[39m urllib\u001b[39m.\u001b[39mrequest\u001b[39m.\u001b[39murlopen(q)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/guus/Documents/Code/is-it-mould/test_scraping.ipynb#ch0000040?line=5'>6</a>\u001b[0m data \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39;49mread()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/guus/Documents/Code/is-it-mould/test_scraping.ipynb#ch0000040?line=6'>7</a>\u001b[0m result \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(data)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/guus/Documents/Code/is-it-mould/test_scraping.ipynb#ch0000040?line=7'>8</a>\u001b[0m ids \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/praw/lib/python3.10/http/client.py:481\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/guus/miniconda3/envs/praw/lib/python3.10/http/client.py?line=478'>479</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/guus/miniconda3/envs/praw/lib/python3.10/http/client.py?line=479'>480</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/guus/miniconda3/envs/praw/lib/python3.10/http/client.py?line=480'>481</a>\u001b[0m         s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_safe_read(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlength)\n\u001b[1;32m    <a href='file:///Users/guus/miniconda3/envs/praw/lib/python3.10/http/client.py?line=481'>482</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m IncompleteRead:\n\u001b[1;32m    <a href='file:///Users/guus/miniconda3/envs/praw/lib/python3.10/http/client.py?line=482'>483</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/miniconda3/envs/praw/lib/python3.10/http/client.py:630\u001b[0m, in \u001b[0;36mHTTPResponse._safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/guus/miniconda3/envs/praw/lib/python3.10/http/client.py?line=622'>623</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_safe_read\u001b[39m(\u001b[39mself\u001b[39m, amt):\n\u001b[1;32m    <a href='file:///Users/guus/miniconda3/envs/praw/lib/python3.10/http/client.py?line=623'>624</a>\u001b[0m     \u001b[39m\"\"\"Read the number of bytes requested.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/guus/miniconda3/envs/praw/lib/python3.10/http/client.py?line=624'>625</a>\u001b[0m \n\u001b[1;32m    <a href='file:///Users/guus/miniconda3/envs/praw/lib/python3.10/http/client.py?line=625'>626</a>\u001b[0m \u001b[39m    This function should be used when <amt> bytes \"should\" be present for\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/guus/miniconda3/envs/praw/lib/python3.10/http/client.py?line=626'>627</a>\u001b[0m \u001b[39m    reading. If the bytes are truly not available (due to EOF), then the\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/guus/miniconda3/envs/praw/lib/python3.10/http/client.py?line=627'>628</a>\u001b[0m \u001b[39m    IncompleteRead exception can be used to detect the problem.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/guus/miniconda3/envs/praw/lib/python3.10/http/client.py?line=628'>629</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/guus/miniconda3/envs/praw/lib/python3.10/http/client.py?line=629'>630</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mread(amt)\n\u001b[1;32m    <a href='file:///Users/guus/miniconda3/envs/praw/lib/python3.10/http/client.py?line=630'>631</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m<\u001b[39m amt:\n\u001b[1;32m    <a href='file:///Users/guus/miniconda3/envs/praw/lib/python3.10/http/client.py?line=631'>632</a>\u001b[0m         \u001b[39mraise\u001b[39;00m IncompleteRead(data, amt\u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(data))\n",
      "File \u001b[0;32m~/miniconda3/envs/praw/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/guus/miniconda3/envs/praw/lib/python3.10/socket.py?line=702'>703</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/guus/miniconda3/envs/praw/lib/python3.10/socket.py?line=703'>704</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/guus/miniconda3/envs/praw/lib/python3.10/socket.py?line=704'>705</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    <a href='file:///Users/guus/miniconda3/envs/praw/lib/python3.10/socket.py?line=705'>706</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    <a href='file:///Users/guus/miniconda3/envs/praw/lib/python3.10/socket.py?line=706'>707</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/praw/lib/python3.10/ssl.py:1273\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/guus/miniconda3/envs/praw/lib/python3.10/ssl.py?line=1268'>1269</a>\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   <a href='file:///Users/guus/miniconda3/envs/praw/lib/python3.10/ssl.py?line=1269'>1270</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   <a href='file:///Users/guus/miniconda3/envs/praw/lib/python3.10/ssl.py?line=1270'>1271</a>\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   <a href='file:///Users/guus/miniconda3/envs/praw/lib/python3.10/ssl.py?line=1271'>1272</a>\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> <a href='file:///Users/guus/miniconda3/envs/praw/lib/python3.10/ssl.py?line=1272'>1273</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   <a href='file:///Users/guus/miniconda3/envs/praw/lib/python3.10/ssl.py?line=1273'>1274</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/guus/miniconda3/envs/praw/lib/python3.10/ssl.py?line=1274'>1275</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/miniconda3/envs/praw/lib/python3.10/ssl.py:1129\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/guus/miniconda3/envs/praw/lib/python3.10/ssl.py?line=1126'>1127</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/guus/miniconda3/envs/praw/lib/python3.10/ssl.py?line=1127'>1128</a>\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///Users/guus/miniconda3/envs/praw/lib/python3.10/ssl.py?line=1128'>1129</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   <a href='file:///Users/guus/miniconda3/envs/praw/lib/python3.10/ssl.py?line=1129'>1130</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/guus/miniconda3/envs/praw/lib/python3.10/ssl.py?line=1130'>1131</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Query last 500 mold results\n",
    "q = 'https://api.pushshift.io/reddit/search/submission/?subreddit=kombucha&sort=desc&sort_type=created_utc&size=500&q=mold'\n",
    "response = urllib.request.urlopen(q)\n",
    "data = response.read()\n",
    "result = json.loads(data)\n",
    "ids = list()\n",
    "for x in result[\"data\"]:\n",
    "    ids.append(x[\"id\"])\n",
    "    date = datetime.datetime.fromtimestamp(x[\"created_utc\"])\n",
    "    print(\"%s\\t%s\\t%s\" % (x[\"id\"], date, x[\"title\"]))\n",
    "\n",
    "print(len(ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the flair text via PRAW\n",
    "full_ids = [i if i.startswith('t3_') else f't3_{i}' for i in ids]\n",
    "posts = reddit.info(full_ids)\n",
    "for post in posts:\n",
    "    print(post.link_flair_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems to work. We first fetched the posts using Pushshift, and then batch queried PRAW using the post ids to get the current flairs. The number of returned items seems to be limited to 100, so we could fetch all the data in batches of 100 using the `before` / `after` filter. Next question: can we fetch posts from further back? to test this, let's sort the posts ascending, rather than descending - this should get us the first ever image posts in the subreddit. Because flairs may not have been in use back then, let's just print the titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 'https://api.pushshift.io/reddit/search/submission/?subreddit=kombucha&sort=asc&sort_type=created_utc&size=5&q=\".jpg\"'\n",
    "response = urllib.request.urlopen(q)\n",
    "data = response.read()\n",
    "result = json.loads(data)\n",
    "ids = list()\n",
    "for x in result[\"data\"]:\n",
    "    ids.append(x[\"id\"])\n",
    "    print(\"%s\\t%s\" % (x[\"id\"], x[\"title\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the post title through PRAW\n",
    "full_ids = [i if i.startswith('t3_') else f't3_{i}' for i in ids]\n",
    "posts = reddit.info(full_ids)\n",
    "for post in posts:\n",
    "    print(post.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That also seems to work. Now we can limit the fields to the essential ones to reduce query time and data transfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"https://api.pushshift.io/reddit/search/submission/?subreddit=kombucha&sort=desc&sort_type=created_utc&size=5&fields=id,media_metadata,created_utc,url,link_flair_text,is_gallery,retrieved_on,title\"\n",
    "response = urllib.request.urlopen(q)\n",
    "data = response.read()\n",
    "result = json.loads(data)\n",
    "ids = list()\n",
    "dates = list()\n",
    "for x in result[\"data\"]:\n",
    "    ids.append(x[\"id\"])\n",
    "    dates.append(x[\"created_utc\"])\n",
    "    print(\"%s\\t%s\\t%s\" % (x[\"id\"], x[\"created_utc\"], x[\"title\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if we can fetch the last 5 posts before a specific post. We select the timestamp from the third post above, and fetch the 5 posts before it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"https://api.pushshift.io/reddit/search/submission/?subreddit=kombucha&sort=desc&sort_type=created_utc&size=5&fields=id,media_metadata,created_utc,url,link_flair_text,retrieved_on,title&before=%s\" % dates[2]\n",
    "response = urllib.request.urlopen(q)\n",
    "data = response.read()\n",
    "result = json.loads(data)\n",
    "ids = list()\n",
    "for x in result[\"data\"]:\n",
    "    ids.append(x[\"id\"])\n",
    "    print(\"%s\\t%s\" % (x[\"id\"], x[\"title\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that we can iteratively fetch 100 posts, get the timestamp of the last post, and then fetch the 100 posts before that, which allow us to scrape all posts in a subreddit. PSAW is a wrapper around pushshift that does exactly that: it paginates by timestamp in blocks of 100, but makes sure that not too many requests are sent and that any timeout is caught. This means that we can fetch the entire history of a subreddit in a single command. Let's try it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psaw import PushshiftAPI\n",
    "api = PushshiftAPI()\n",
    "\n",
    "# Get all posts made between march 8 2022 and now\n",
    "start = int(datetime.datetime(2022, 3, 8).timestamp())\n",
    "res = api.search_submissions(\n",
    "    after = start,\n",
    "    subreddit = 'kombucha',\n",
    "    sort_type = 'desc',\n",
    "    filter = ['id', 'media_metadata', 'created_utc', 'url', 'link_flair_text', 'retrieved_on' , 'title'])\n",
    "posts = list(res)\n",
    "print(len(posts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems to run forever, even for a short timespan. It may be better to implement this function manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts fetched: 100\n",
      "Number of posts fetched: 164\n"
     ]
    }
   ],
   "source": [
    "# We expect 100 hits per query\n",
    "limit = 100\n",
    "n_hits = limit\n",
    "\n",
    "# Get the current timestamp\n",
    "now = datetime.datetime.today()\n",
    "now_ts = datetime.datetime.timestamp(now)\n",
    "ts = int(now_ts)\n",
    "\n",
    "# Get a date at which to stop fetching posts\n",
    "end_str = \"03/01/2022\"\n",
    "end = datetime.datetime.strptime(end_str, \"%m/%d/%Y\")\n",
    "end_ts = int(datetime.datetime.timestamp(end))\n",
    "\n",
    "# Structure to store submissions\n",
    "posts = []\n",
    "\n",
    "# Rudimentary function to call the API\n",
    "def fetch_ps_submission(\n",
    "    ts, end_ts, limit = 100):\n",
    "    q = \"https://api.pushshift.io/reddit/search/submission/?subreddit=kombucha&sort=desc&sort_type=created_utc&size=%s&fields=id,media_metadata,created_utc,url,link_flair_text,retrieved_on,title&before=%s&after=%s\" % (limit, ts, end_ts)\n",
    "    response = urllib.request.urlopen(q)\n",
    "    data = response.read()\n",
    "    result = json.loads(data)[\"data\"]\n",
    "    return(result)\n",
    "\n",
    "# Fetch results until the end date is reached or results run out\n",
    "while (ts >= end_ts and n_hits == limit):\n",
    "    result = fetch_ps_submission(ts, end_ts)\n",
    "    posts = posts + result\n",
    "    print(\"Number of posts fetched: \" + str(len(posts)))\n",
    "    n_hits = len(result)\n",
    "    ts = int(result[n_hits - 1][\"created_utc\"])\n",
    "    time.sleep(1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to fetch all submissions up to a specific date, we can store the submissions based on the presence of `.jp(e)g` in the post URL or `media_metadata`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Submission:\n",
    "\n",
    "    # Base attributes imported from pushshift\n",
    "    def __init__(self, ps_out):\n",
    "\n",
    "        # PushShift parameters\n",
    "        self.id = ps_out[\"id\"]\n",
    "        self.url = ps_out[\"url\"]\n",
    "        self.created = ps_out[\"created_utc\"]\n",
    "        self.retrieved = ps_out[\"retrieved_on\"]\n",
    "        self.title = ps_out[\"title\"]\n",
    "        self.media_meta = ps_out[\"media_metadata\"] if \"media_metadata\" in ps_out.keys() else None\n",
    "        self.original_flair = ps_out[\"link_flair_text\"] if \"link_flair_text\" in ps_out.keys() else None\n",
    "    \n",
    "        # Inferred parameters\n",
    "        self.has_image = False\n",
    "        self.img_urls = []\n",
    "        self.dl_imgs = False\n",
    "\n",
    "        # PRAW parameters\n",
    "        self.current_flair = None\n",
    "\n",
    "    # Method to check for images\n",
    "    def check_image(self):\n",
    "        if self.url.endswith(('.jpg', '.jpeg')):\n",
    "            self.has_image = True\n",
    "            self.img_urls.append(self.url)\n",
    "        elif self.media_meta is not None:\n",
    "            self.has_image = True\n",
    "            for id in self.media_meta.keys():\n",
    "                try:\n",
    "                    url = self.media_meta[id][\"s\"][\"u\"]\n",
    "                    self.img_urls.append(url)\n",
    "                except KeyError:\n",
    "                    # Not all images are processed and therefore lack the \"s\" key\n",
    "                    continue\n",
    "    \n",
    "    # Method to check if post has a relevant flair for downloading\n",
    "    def check_download(self, flairs):\n",
    "        if self.current_flair in flairs:\n",
    "            self.dl_imgs = True\n",
    "\n",
    "    # Method to extract additional PRAW parameters\n",
    "    def get_praw(self, praw_subm):\n",
    "        self.media_meta = praw_subm.media_metadata if \"media_metadata\" in vars(praw_subm) else None\n",
    "        self.current_flair = praw_subm.link_flair_text if \"link_flair_text\" in vars(praw_subm) else None\n",
    "        self.img_urls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://preview.redd.it/d8oy2jf30cm81.jpg?width=3072&amp;format=pjpg&amp;auto=webp&amp;s=69d2aa30768fe421003a4115917d0f697265dd01', 'https://preview.redd.it/f4ylvdl30cm81.jpg?width=3072&amp;format=pjpg&amp;auto=webp&amp;s=151003e4448303a32e37809c02e3bd6d7b18785f']\n",
      "['https://i.redd.it/4o54mmxtl6m81.jpg']\n"
     ]
    }
   ],
   "source": [
    "# Store all submissions into a dictionary of objects\n",
    "submissions = dict()\n",
    "for p in posts:\n",
    "    sub = Submission(p)\n",
    "    sub.check_image()\n",
    "    submissions[sub.id] = sub\n",
    "\n",
    "print(submissions[\"ta4k3h\"].img_urls)\n",
    "print(submissions[\"t9js4w\"].img_urls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above I printed two URLs from the `submission.media_metadata`, and one from a `submission.url`. The ones from the `media_metadata` do not appear to be working, so we will also have to fetch those through PRAW. Let's create the PRAW fetching part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://preview.redd.it/f4ylvdl30cm81.jpg?width=3072&format=pjpg&auto=webp&s=151003e4448303a32e37809c02e3bd6d7b18785f', 'https://preview.redd.it/d8oy2jf30cm81.jpg?width=3072&format=pjpg&auto=webp&s=69d2aa30768fe421003a4115917d0f697265dd01']\n",
      "['https://i.redd.it/4o54mmxtl6m81.jpg']\n",
      "beautiful booch\n",
      "beautiful booch\n"
     ]
    }
   ],
   "source": [
    "# Get the ids of posts with images\n",
    "ids = submissions.keys()\n",
    "img_ids = []\n",
    "for id in ids:\n",
    "    if submissions[id].has_image:\n",
    "        img_ids.append(id)\n",
    "full_ids = [i if i.startswith('t3_') else f't3_{i}' for i in img_ids]\n",
    "\n",
    "# Split the ids into chunks\n",
    "# courtesy of https://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks\n",
    "def chunks(l, n):\n",
    "    n = max(1, n)\n",
    "    return (l[i:i+n] for i in range(0, len(l), n))\n",
    "ids_list = chunks(full_ids, 50)\n",
    "\n",
    "# Loop through chunks and obtain submissions through PRAW\n",
    "for x in ids_list:\n",
    "    posts = reddit.info(x)\n",
    "    for post in posts:\n",
    "        submissions[post.id].get_praw(post)\n",
    "        submissions[post.id].check_image()\n",
    "\n",
    "print(submissions[\"ta4k3h\"].img_urls)\n",
    "print(submissions[\"t9js4w\"].img_urls)\n",
    "print(submissions[\"t9js4w\"].original_flair)\n",
    "print(submissions[\"t9js4w\"].current_flair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! We now have the original and current flairs, as well as working URLs to images. Now all that's left is to subset the data to posts that included a mold related flair and download the images. Let's also write the data to a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mold! mold! https://www.reddit.com/gallery/t9qzee\n",
      "mold! mold! https://i.redd.it/dzvsztbe61m81.jpg\n",
      "kahm! kahm! https://i.redd.it/ekg4osqg21m81.jpg\n",
      "what's wrong!? not mold https://www.reddit.com/gallery/t86y7v\n",
      "question not mold https://www.reddit.com/gallery/t6a39m\n",
      "mold! mold! https://www.reddit.com/gallery/t4k01s\n"
     ]
    }
   ],
   "source": [
    "# Check if a mould flair is present and whether it has changed\n",
    "for sub in submissions.values():\n",
    "    flairs = ['mold!', 'kahm!', 'not mold', 'pellicle']\n",
    "    sub.check_download(flairs)\n",
    "    if (sub.dl_imgs):\n",
    "        print(sub.original_flair, sub.current_flair, sub.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that out of the recent posts, most started with a mold flair already assigned. Upon inspecting the images, they also appear to be correctly assigned. For the moment, let's not exclude those posts and first scrape all the data. We can always exclude those images later when we are certain that there are enough images for our classifier."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "74597b8a6c2e78b1702df7d723b5e614cf34ec9e123fac975035cbc72d20fe8a"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('3.7.3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
